{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1adb4ab",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, using BBRL, we code the DDPG algorithm.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The DDPG algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=0D6a0a1HTtc) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ddpg.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2305b55",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rh4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl_utils\\notebook.py:46: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # noqa: F401\n"
     ]
    }
   ],
   "source": [
    "# Prepare the environment\n",
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import bbrl_gymnasium  # noqa: F401\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.visu.plot_policies import plot_policy\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812a713",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1d8b54",
   "metadata": {},
   "source": [
    "The [DDPG](https://arxiv.org/pdf/1509.02971.pdf) algorithm is an actor critic\n",
    "algorithm. We use a simple neural network builder function. This neural\n",
    "networks plays the role of the actor and the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae68782",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Definition of agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ea0e5",
   "metadata": {},
   "source": [
    "The critic is a neural network taking the state $s$ and action $a$ as input,\n",
    "and its output layer has a unique neuron whose value is the value of being in\n",
    "that state and performing that action $Q(s,a)$.\n",
    "\n",
    "As usual, the ```forward(...)``` function is used to write Q-values in the\n",
    "workspace from time indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2ef477",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # Get the current state $s_t$ and the chosen action $a_t$\n",
    "        obs = self.get((\"env/env_obs\", t))  # shape B x D_{obs}\n",
    "        action = self.get((\"action\", t))  # shape B x D_{action}\n",
    "\n",
    "        # Compute the Q-value(s_t, a_t)\n",
    "        obs_act = torch.cat((obs, action), dim=1)  # shape B x (D_{obs} + D_{action})\n",
    "        # Get the q-value (and remove the last dimension since it is a scalar)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37193038",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The actor is also a neural network, it takes a state $s$ as input and outputs\n",
    "an action $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba4b199",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousDeterministicActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim):\n",
    "        super().__init__()\n",
    "        layers = [state_dim] + list(hidden_layers) + [action_dim]\n",
    "        self.model = build_mlp(\n",
    "            layers, activation=nn.ReLU(), output_activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.model(obs)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99669344",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Exploration method\n",
    "\n",
    "In the continuous action domain, basic exploration differs from the methods\n",
    "used in the discrete action domain. Here we generally add some Gaussian noise\n",
    "to the output of the actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70d9091",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(Agent):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        dist = Normal(act, self.sigma)\n",
    "        action = dist.sample()\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a29ed5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In [the original DDPG paper](https://arxiv.org/pdf/1509.02971.pdf), the\n",
    "authors rather used the more sophisticated Ornstein-Uhlenbeck noise where\n",
    "noise is correlated between one step and the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9cfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddOUNoise(Agent):\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process noise for actions as suggested by DDPG paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, std_dev, theta=0.15, dt=1e-2):\n",
    "        self.theta = theta\n",
    "        self.std_dev = std_dev\n",
    "        self.dt = dt\n",
    "        self.x_prev = 0\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        act = self.get((\"action\", t))\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (act - self.x_prev) * self.dt\n",
    "            + self.std_dev * math.sqrt(self.dt) * torch.randn(act.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        self.set((\"action\", t), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978fe28",
   "metadata": {},
   "source": [
    "### Compute critic loss\n",
    "\n",
    "Detailed explanations of the function to compute the critic loss\n",
    "are given in [the DQN notebook](http://master-dac.isir.upmc.fr/rld/rl/03-1-dqn-introduction.student.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f2e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the (Torch) mse loss function\n",
    "# `mse(x, y)` computes $\\|x-y\\|^2$\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "def compute_critic_loss(cfg, reward, must_bootstrap, q_values, target_q_values):\n",
    "    \"\"\"Compute the DDPG critic loss from a sample of transitions\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: The reward (shape 2xB)\n",
    "    :param must_bootstrap: Must bootstrap flag (shape 2xB)\n",
    "    :param q_values: The computed Q-values (shape 2xB)\n",
    "    :param target_q_values: The Q-values computed by the target critic (shape 2xB)\n",
    "    :return: the loss (a scalar)\n",
    "    \"\"\"\n",
    "    # Compute temporal difference\n",
    "    target = (\n",
    "        reward[1]\n",
    "        + cfg.algorithm.discount_factor * target_q_values[1] * must_bootstrap[1].int()\n",
    "    )\n",
    "    # Compute critic loss\n",
    "    critic_loss = mse(q_values[0], target)\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbbd42",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute actor loss\n",
    "The actor loss is straightforward. We want the actor to maximize Q-values, thus we minimize the mean of negated Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2553710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(q_values):\n",
    "    \"\"\"Returns the actor loss\n",
    "\n",
    "    :param q_values: The q-values (shape 2xB)\n",
    "    :return: A scalar (the loss)\n",
    "    \"\"\"\n",
    "    return -q_values[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c20be8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c0fd5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeec9de0",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "\n",
    "In the following, we code the main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2eec04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5efd1330",
   "metadata": {},
   "source": [
    "# Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdee322d-ab5c-4567-ad9c-3d77e6867389",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import Box2D\n",
    "    from Box2D.b2 import (\n",
    "        circleShape,\n",
    "        contactListener,\n",
    "        edgeShape,\n",
    "        fixtureDef,\n",
    "        polygonShape,\n",
    "        revoluteJointDef,\n",
    "    )\n",
    "except ImportError as e:\n",
    "    raise DependencyNotInstalled(\n",
    "        'Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`'\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcd017",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdefec3-c54e-406a-b8bb-c4231047d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to launch tensorboard in this notebook [y/n]  y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4488), started 1:44:14 ago. (Use '!kill 4488' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e9a5ce3ed12d6704\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e9a5ce3ed12d6704\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7791309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12eaf381",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Starting from the above version , you should code [the TD3\n",
    "algorithm](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf).\n",
    "\n",
    "## Differences with DDPG\n",
    "\n",
    "### Target policy smoothing\n",
    "\n",
    "The noise is clipped\n",
    "\n",
    "### Clipped double-Q learning\n",
    "\n",
    "$$y(r,s',d) = r + \\gamma (1 - d) \\min_{i=1,2} Q_{\\phi_{i, \\text{targ}}}(s', a'(s'))$$\n",
    "\n",
    "where both Q-value estimators are learned using the following loss:\n",
    "\n",
    "$$L(\\phi_1, {\\mathcal D}) = \\mathbb{E}_{(s,a,r,s',d) \\sim {\\mathcal D}}{\\Bigg( Q_{\\phi_1}(s,a) - y(r,s',d) \\Bigg)^2}$$\n",
    "$$L(\\phi_2, {\\mathcal D}) = \\mathbb{E}_{(s,a,r,s',d) \\sim {\\mathcal D}}{\\Bigg( Q_{\\phi_2}(s,a) - y(r,s',d) \\Bigg)^2}.$$\n",
    "\n",
    "### Policy learning\n",
    "\n",
    "We learn the policy with using the first critic ($\\Phi_1$)\n",
    "$$\\max_{\\theta} \\underset{s \\sim {\\mathcal D}}{{\\mathrm E}}\\left[ Q_{\\phi_1}(s, \\mu_{\\theta}(s)) \\right]$$\n",
    "\n",
    "## Algorithm details\n",
    "\n",
    "For that, you need to use two critics (and two target critics) and always take\n",
    "the minimum output between the two when you ask for the Q-value of a (state,\n",
    "action) pair.\n",
    "\n",
    "In more detail, you have to do the following:\n",
    "- replace the single critic and corresponding target critic with two critics\n",
    "  and target critics (name them ```critic_1, critic_2, target_critic_1,\n",
    "  target_critic_2```)\n",
    "- get the q-values and target q-values corresponding to all these critics.\n",
    "- then the target q-values you should consider to update the critic should be\n",
    "  the minimum over the target q-values at each step (use ```torch.min(...)``` to\n",
    "  get this min over a sequence of data).\n",
    "- to update the actor, do it with the q-values of an arbitrarily chosen\n",
    "  critic, e.g. critic_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1299fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # we create the critic and the actor, but also an exploration agent to\n",
    "        # add noise and a target critic. The version below does not use a target\n",
    "        # actor as it proved hard to tune, but such a target actor is used in\n",
    "        # the original paper.\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        \n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_1/\")\n",
    "\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size, act_size\n",
    "        ).with_prefix(\"critic_2/\")\n",
    "\n",
    "\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\"target_critic_1/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\"target_critic_2/\")\n",
    "\n",
    "\n",
    "        self.actor = ContinuousDeterministicActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # As an alternative, you can use `AddOUNoise`\n",
    "        noise_agent = AddGaussianNoise(cfg.algorithm.action_noise)\n",
    "\n",
    "        self.train_policy = Agents(self.actor, noise_agent)\n",
    "        self.eval_policy = self.actor\n",
    "\n",
    "        # Define agents over time\n",
    "        self.t_actor = TemporalAgent(self.actor)\n",
    "        self.t_critic_1 = TemporalAgent(self.critic_1)\n",
    "        self.t_target_critic_1 = TemporalAgent(self.target_critic_1)\n",
    "\n",
    "        self.t_critic_2 = TemporalAgent(self.critic_2)\n",
    "        self.t_target_critic_2 = TemporalAgent(self.target_critic_2)\n",
    "\n",
    "        # Configure the optimizer\n",
    "        self.actor_optimizer = setup_optimizer(cfg.actor_optimizer, self.actor)\n",
    "        self.critic_optimizer = setup_optimizer(cfg.critic_optimizer, self.critic_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_td3(td3: TD3):\n",
    "    for rb in td3.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(td3.cfg.algorithm.batch_size)\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "\n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # Critic update\n",
    "        # compute q_values: at t, we have Q(s,a) from the (s,a) in the RB\n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "        td3.t_critic_2(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # replace the action aRB with \\pi(s_{t+1}), to compute\n",
    "            # Q(s_{t+1}, \\pi(s_{t+1}) below\n",
    "            td3.t_actor(rb_workspace, t=1, n_steps=1)\n",
    "            # compute q_values: at t+1 we have Q(s_{t+1}, \\pi(s_{t+1})\n",
    "            td3.t_target_critic_1(rb_workspace, t=1, n_steps=1)\n",
    "            td3.t_target_critic_2(rb_workspace, t=1, n_steps=1)\n",
    "\n",
    "\n",
    "        # finally q_values contains the above collection at t=0 and t=1\n",
    "        q_values_1, post_q_values_1, q_values_2, post_q_values_2 = rb_workspace[\n",
    "            \"critic_1/q_value\", \"target_critic_1/q_value\",\n",
    "            \"critic_2/q_value\", \"target_critic_2/q_value\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        post_q_values = torch.min(post_q_values_1, post_q_values_2)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = compute_critic_loss(\n",
    "            td3.cfg, reward, must_bootstrap, q_values_1, post_q_values\n",
    "        )\n",
    "\n",
    "        td3.logger.add_log(\"critic_loss\", critic_loss, td3.nb_steps)\n",
    "        td3.critic_optimizer.zero_grad()\n",
    "\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.critic_1.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        \n",
    "        td3.critic_optimizer.step()\n",
    "\n",
    "        # Actor update\n",
    "\n",
    "        # Now we determine the actions the current policy would take in the states from the RB\n",
    "        td3.t_actor(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        # We determine the Q values resulting from actions of the current policy\n",
    "        td3.t_critic_1(rb_workspace, t=0, n_steps=1)\n",
    "\n",
    "        # and we back-propagate the corresponding loss to maximize the Q values\n",
    "        q_values = rb_workspace[\"critic_1/q_value\"]\n",
    "        actor_loss = compute_actor_loss(q_values)\n",
    "\n",
    "        td3.logger.add_log(\"actor_loss\", actor_loss, td3.nb_steps)\n",
    "\n",
    "        # if -25 < actor_loss < 0 and nb_steps > 2e5:\n",
    "        td3.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            td3.actor.parameters(), td3.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        td3.actor_optimizer.step()\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(\n",
    "            td3.critic_1, td3.target_critic_1, td3.cfg.algorithm.tau_target\n",
    "        )\n",
    "\n",
    "        if td3.evaluate():\n",
    "            if td3.cfg.plot_agents:\n",
    "                plot_policy(\n",
    "                    td3.actor,\n",
    "                    td3.eval_env,\n",
    "                    td3.best_reward,\n",
    "                    str(td3.base_dir / \"plots\"),\n",
    "                    td3.cfg.gym_env.env_name,\n",
    "                    stochastic=False,\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d00657",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a79c63435174cbf809a087ec799747e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Converting from sequence to b2Vec2, expected int/float arguments index 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 43\u001B[0m\n\u001B[0;32m      1\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_best\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;132;01m{gym_env.env_name}\u001B[39;00m\u001B[38;5;124m/td3-S$\u001B[39m\u001B[38;5;132;01m{algorithm.seed}\u001B[39;00m\u001B[38;5;124m_$\u001B[39m\u001B[38;5;132;01m{current_time:}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     39\u001B[0m     },\n\u001B[0;32m     40\u001B[0m }\n\u001B[0;32m     42\u001B[0m td3 \u001B[38;5;241m=\u001B[39m TD3(OmegaConf\u001B[38;5;241m.\u001B[39mcreate(params))\n\u001B[1;32m---> 43\u001B[0m \u001B[43mrun_td3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd3\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m td3\u001B[38;5;241m.\u001B[39mvisualize_best()\n",
      "Cell \u001B[1;32mIn[13], line 52\u001B[0m, in \u001B[0;36mrun_td3\u001B[1;34m(td3)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_td3\u001B[39m(td3: TD3):\n\u001B[1;32m---> 52\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtd3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_replay_buffers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrb_workspace\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_shuffled\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malgorithm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mterminated\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrb_workspace\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43menv/terminated\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43menv/reward\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl_utils\\algorithms.py:281\u001B[0m, in \u001B[0;36mEpochBasedAlgo.iter_replay_buffers\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m epochs_pb:\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;66;03m# This is the tricky part with transition buffers. The difficulty lies in the\u001B[39;00m\n\u001B[0;32m    263\u001B[0m     \u001B[38;5;66;03m# copy of the last step and the way to deal with the n_steps return.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;66;03m# epoch. This is explained in more details in [a previous\u001B[39;00m\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;66;03m# notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\u001B[39;00m\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;66;03m# First run: we start from scratch\u001B[39;00m\n\u001B[1;32m--> 281\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_agent\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtrain_workspace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m            \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malgorithm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstochastic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    288\u001B[0m         \u001B[38;5;66;03m# Other runs: we copy the last step and start from there\u001B[39;00m\n\u001B[0;32m    289\u001B[0m         train_workspace\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl\\agents\\utils.py:79\u001B[0m, in \u001B[0;36mTemporalAgent.__call__\u001B[1;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001B[0m\n\u001B[0;32m     77\u001B[0m _t \u001B[38;5;241m=\u001B[39m t\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 79\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_variable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     81\u001B[0m         s \u001B[38;5;241m=\u001B[39m workspace\u001B[38;5;241m.\u001B[39mget(stop_variable, _t)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl\\agents\\utils.py:31\u001B[0m, in \u001B[0;36mAgents.__call__\u001B[1;34m(self, workspace, **kwargs)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, workspace, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magents:\n\u001B[1;32m---> 31\u001B[0m         \u001B[43ma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworkspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl\\agents\\agent.py:84\u001B[0m, in \u001B[0;36mAgent.__call__\u001B[1;34m(self, workspace, **kwargs)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m workspace \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Agent.__call__] workspace must not be None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkspace \u001B[38;5;241m=\u001B[39m workspace\n\u001B[1;32m---> 84\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkspace \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl\\agents\\gymnasium.py:480\u001B[0m, in \u001B[0;36mParallelGymAgent.forward\u001B[1;34m(self, t, **kwargs)\u001B[0m\n\u001B[0;32m    478\u001B[0m         frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step(k, dict_slice(k, action))\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 480\u001B[0m         frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    482\u001B[0m     \u001B[38;5;66;03m# Use last frame\u001B[39;00m\n\u001B[0;32m    483\u001B[0m     frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_frame[k]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bbrl\\agents\\gymnasium.py:428\u001B[0m, in \u001B[0;36mParallelGymAgent._step\u001B[1;34m(self, k, action)\u001B[0m\n\u001B[0;32m    425\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvs[k]\n\u001B[0;32m    427\u001B[0m action: Union[\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray[\u001B[38;5;28mint\u001B[39m]] \u001B[38;5;241m=\u001B[39m _convert_action(action)\n\u001B[1;32m--> 428\u001B[0m obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timestep[k] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcumulated_reward[k] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\autoreset.py:56\u001B[0m, in \u001B[0;36mAutoResetWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     48\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment with action and resets the environment if a terminated or truncated signal is encountered.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03m        The autoreset environment :meth:`step`\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 56\u001B[0m     obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[0;32m     58\u001B[0m         new_obs, new_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     55\u001B[0m \n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:578\u001B[0m, in \u001B[0;36mLunarLander.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    564\u001B[0m         p \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_particle(\n\u001B[0;32m    565\u001B[0m             \u001B[38;5;241m3.5\u001B[39m,  \u001B[38;5;66;03m# 3.5 is here to make particle speed adequate\u001B[39;00m\n\u001B[0;32m    566\u001B[0m             impulse_pos[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    567\u001B[0m             impulse_pos[\u001B[38;5;241m1\u001B[39m],\n\u001B[0;32m    568\u001B[0m             m_power,\n\u001B[0;32m    569\u001B[0m         )\n\u001B[0;32m    570\u001B[0m         p\u001B[38;5;241m.\u001B[39mApplyLinearImpulse(\n\u001B[0;32m    571\u001B[0m             (\n\u001B[0;32m    572\u001B[0m                 ox \u001B[38;5;241m*\u001B[39m MAIN_ENGINE_POWER \u001B[38;5;241m*\u001B[39m m_power,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    576\u001B[0m             \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    577\u001B[0m         )\n\u001B[1;32m--> 578\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlander\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mApplyLinearImpulse\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mox\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mMAIN_ENGINE_POWER\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mm_power\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43moy\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mMAIN_ENGINE_POWER\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mm_power\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    580\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimpulse_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    581\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    584\u001B[0m s_power \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontinuous \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39mabs(action[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontinuous \u001B[38;5;129;01mand\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m    587\u001B[0m ):\n\u001B[0;32m    588\u001B[0m     \u001B[38;5;66;03m# Orientation/Side engines\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: Converting from sequence to b2Vec2, expected int/float arguments index 0"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"save_best\": False,\n",
    "    \"base_dir\": \"${gym_env.env_name}/td3-S${algorithm.seed}_${current_time:}\",\n",
    "    \"collect_stats\": True,\n",
    "    # Set to true to have an insight on the learned policy\n",
    "    # (but slows down the evaluation a lot!)\n",
    "    \"plot_agents\": True,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"epsilon\": 0.02,\n",
    "        \"n_envs\": 1,\n",
    "        \"n_steps\": 100,\n",
    "        \"nb_evals\": 10,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"buffer_size\": 2e5,\n",
    "        \"batch_size\": 64,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"max_epochs\": 11_000,\n",
    "        # Minimum number of transitions before learning starts\n",
    "        \"learning_starts\": 10000,\n",
    "        \"action_noise\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [400, 300],\n",
    "            \"critic_hidden_size\": [400, 300],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"LunarLanderContinuous-v2\",\n",
    "    },\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "}\n",
    "\n",
    "td3 = TD3(OmegaConf.create(params))\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2a5f5",
   "metadata": {},
   "source": [
    "## Experimental comparison\n",
    "\n",
    "Take an environment where the over-estimation bias may matter, and compare the\n",
    "performance of DDPG and TD3. Visualize the Q-value long before convergence to\n",
    "see whether indeed DDPG overestimates the Q-values with respect to TD3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbrl.stats import WelchTTest\n",
    "\n",
    "WelchTTest().plot(\n",
    "    torch.stack(ddpg.eval_rewards),\n",
    "    torch.stack(td3.eval_rewards),\n",
    "    legends=\"ddpg/td3\",\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1caf52-d351-405e-9eb4-891f3954c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
